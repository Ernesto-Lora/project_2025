{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e649eeb2-1c31-4dc4-bf56-a7ba54bd6080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pgmpy.readwrite import XMLBIFReader\n",
    "import random\n",
    "import copy\n",
    "\n",
    "# Settings\n",
    "from discriminativeBN import DiscriminativeBN\n",
    "from simplexMethod import NelderMeadOptimizer\n",
    "from hookeJeeves import HookeJeevesOptimizer\n",
    "from kfold_runner import k_fold_validation\n",
    "from kfold_runner_lbfgs import k_fold_validation_gradient\n",
    "from lbfgs import LBFGSOptimizerWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96aa5865-a610-4426-baf8-28231365b3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled 1 experiments.\n",
      "\n",
      "--- Starting Experiment: Australian - L-BFGS (Gradient) ---\n",
      "Starting 10-Fold Validation (Gradient-Based) on datasets/AustralianDisc.csv...\n",
      "Optimizer: LBFGSOptimizerWrapper | Max Iter: 100\n",
      "--- Starting L-BFGS Optimization (Dim: 118) ---\n",
      "Initial Best Score: -418.9833 (CLL)\n",
      "------------------------------\n",
      "------------------------------\n",
      "Optimization Success!\n",
      "Total Iterations: 9\n",
      "Final Best Score: -175.2017 (CLL)\n",
      "1 fold 86.96%\n",
      "--- Starting L-BFGS Optimization (Dim: 118) ---\n",
      "Initial Best Score: -394.6773 (CLL)\n",
      "------------------------------\n",
      "------------------------------\n",
      "Optimization Success!\n",
      "Total Iterations: 9\n",
      "Final Best Score: -182.4409 (CLL)\n",
      "2 fold 89.86%\n",
      "--- Starting L-BFGS Optimization (Dim: 118) ---\n",
      "Initial Best Score: -481.4228 (CLL)\n",
      "------------------------------\n",
      "------------------------------\n",
      "Optimization Success!\n",
      "Total Iterations: 9\n",
      "Final Best Score: -183.7567 (CLL)\n",
      "3 fold 88.41%\n",
      "--- Starting L-BFGS Optimization (Dim: 118) ---\n",
      "Initial Best Score: -466.3629 (CLL)\n",
      "------------------------------\n",
      "------------------------------\n",
      "Optimization Success!\n",
      "Total Iterations: 9\n",
      "Final Best Score: -180.3821 (CLL)\n",
      "4 fold 81.16%\n",
      "--- Starting L-BFGS Optimization (Dim: 118) ---\n",
      "Initial Best Score: -485.7705 (CLL)\n",
      "------------------------------\n",
      "------------------------------\n",
      "Optimization Success!\n",
      "Total Iterations: 8\n",
      "Final Best Score: -190.0233 (CLL)\n",
      "5 fold 89.86%\n",
      "--- Starting L-BFGS Optimization (Dim: 118) ---\n",
      "Initial Best Score: -400.0968 (CLL)\n",
      "------------------------------\n",
      "------------------------------\n",
      "Optimization Success!\n",
      "Total Iterations: 8\n",
      "Final Best Score: -187.0803 (CLL)\n",
      "6 fold 88.41%\n",
      "--- Starting L-BFGS Optimization (Dim: 118) ---\n",
      "Initial Best Score: -438.4068 (CLL)\n",
      "------------------------------\n",
      "------------------------------\n",
      "Optimization Success!\n",
      "Total Iterations: 9\n",
      "Final Best Score: -183.3978 (CLL)\n",
      "7 fold 89.86%\n",
      "--- Starting L-BFGS Optimization (Dim: 118) ---\n",
      "Initial Best Score: -487.3015 (CLL)\n",
      "------------------------------\n",
      "------------------------------\n",
      "Optimization Success!\n",
      "Total Iterations: 8\n",
      "Final Best Score: -175.5724 (CLL)\n",
      "8 fold 79.71%\n",
      "--- Starting L-BFGS Optimization (Dim: 118) ---\n",
      "Initial Best Score: -452.5224 (CLL)\n",
      "------------------------------\n",
      "------------------------------\n",
      "Optimization Success!\n",
      "Total Iterations: 7\n",
      "Final Best Score: -181.1657 (CLL)\n",
      "9 fold 88.41%\n",
      "--- Starting L-BFGS Optimization (Dim: 118) ---\n",
      "Initial Best Score: -482.7502 (CLL)\n",
      "------------------------------\n",
      "------------------------------\n",
      "Optimization Success!\n",
      "Total Iterations: 9\n",
      "Final Best Score: -188.5304 (CLL)\n",
      "10 fold 89.86%\n",
      "\n",
      "average 87.25%\n",
      "Successfully finished Australian - L-BFGS (Gradient).\n",
      "Saved to: results_AustralianNAIVE_LBFGS.txt\n",
      "Elapsed time: 0.24 minutes\n",
      "\n",
      "\n",
      "All scheduled experiments have been processed.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import logging\n",
    "\n",
    "# Set up basic logging to track failures\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "# 1. SETUP: Define your Datasets, Models, and Target Variables here.\n",
    "# IMPORTANT: Check \"target_var\" for diabetes, heart, etc. matches your CSV headers.\n",
    "data_configs = [\n",
    "    {    \"name\": \"Australian\",\n",
    "        \"csv\": \"datasets/AustralianDisc.csv\",\n",
    "        \"xml\": \"models/autralianNaive.xml\",\n",
    "        \"target_var\": \"A15\" \n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# 2. SETUP: Define your Algorithms\n",
    "optimizers = [\n",
    "    {\n",
    "        \"name\": \"L-BFGS (Gradient)\",\n",
    "        \"func\": k_fold_validation_gradient,   # Gradient specific function\n",
    "        \"Optimizer_fun\": LBFGSOptimizerWrapper,\n",
    "        \"max_iter\": 100,\n",
    "        \"file_suffix\": \"LBFGS\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 3. BUILD: Generate the list of experiments dynamically\n",
    "experiments = []\n",
    "\n",
    "for data in data_configs:\n",
    "    for opt in optimizers:\n",
    "        \n",
    "        # Auto-generate the results filename based on dataset and method\n",
    "        # Example: results_AustralianTAN_simplex.txt\n",
    "        result_name = f\"results_{data['name']}NAIVE_{opt['file_suffix']}.txt\"\n",
    "        \n",
    "        experiments.append({\n",
    "            \"name\": f\"{data['name']} - {opt['name']}\",\n",
    "            \"func\": opt['func'],\n",
    "            \"params\": {\n",
    "                \"xml_file\": data['xml'],\n",
    "                \"csv_file\": data['csv'],\n",
    "                \"target_var\": data['target_var'],\n",
    "                \"Optimizer_fun\": opt['Optimizer_fun'],\n",
    "                \"max_iter\": opt['max_iter'],\n",
    "                \"results_file\": result_name\n",
    "            }\n",
    "        })\n",
    "\n",
    "# 4. EXECUTE: Run the loop\n",
    "print(f\"Scheduled {len(experiments)} experiments.\\n\")\n",
    "\n",
    "for exp in experiments:\n",
    "    print(f\"--- Starting Experiment: {exp['name']} ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Unpack the dictionary parameters into the function\n",
    "        exp['func'](**exp['params'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR in {exp['name']}: {e}\")\n",
    "        logging.error(f\"Failed {exp['name']}: {e}\")\n",
    "        # 'continue' jumps to the next iteration of the for-loop\n",
    "        continue \n",
    "    \n",
    "    else:\n",
    "        # This only runs if the 'try' block succeeded\n",
    "        end_time = time.time()\n",
    "        elapsed_minutes = (end_time - start_time) / 60\n",
    "        print(f\"Successfully finished {exp['name']}.\")\n",
    "        print(f\"Saved to: {exp['params']['results_file']}\")\n",
    "        print(f\"Elapsed time: {elapsed_minutes:.2f} minutes\\n\")\n",
    "\n",
    "print(\"\\nAll scheduled experiments have been processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99b013b5-f3ff-4a6b-b669-b81810e278f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_configs = [\n",
    "    {    \"name\": \"Australian\",\n",
    "        \"csv\": \"datasets/AustralianDisc.csv\",\n",
    "        \"xml\": \"models/australianTAN.xml\",\n",
    "        \"target_var\": \"A15\" \n",
    "    },\n",
    "    {    \"name\": \"Heart\",\n",
    "        \"csv\": \"datasets/heartDisc.csv\",\n",
    "        \"xml\": \"models/heartTAN.xml\",\n",
    "        \"target_var\": \"class\" \n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Diabetes\",\n",
    "        \"csv\": \"datasets/diabetesDisc.csv\",\n",
    "        \"xml\": \"models/diabetesTAN.xml\",\n",
    "        \"target_var\": \"Class\" \n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Pima\",\n",
    "        \"csv\": \"datasets/pimaDisc.csv\",\n",
    "        \"xml\": \"models/pimaTAN.xml\",\n",
    "        \"target_var\": \"Class\" \n",
    "    },\n",
    "    {\n",
    "        \"name\": \"TicTacToe\",\n",
    "        \"csv\": \"datasets/tic-tac-toe.csv\",\n",
    "        \"xml\": \"models/ticTAcToeTAN.xml\",\n",
    "        \"target_var\": \"Classs\"\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizers = [\n",
    "    {\n",
    "        \"name\": \"Nelder-Mead\",\n",
    "        \"func\": k_fold_validation,            # Standard function\n",
    "        \"Optimizer_fun\": NelderMeadOptimizer,\n",
    "        \"max_iter\": 6000,\n",
    "        \"file_suffix\": \"simplex\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Hooke-Jeeves\",\n",
    "        \"func\": k_fold_validation,            # Standard function\n",
    "        \"Optimizer_fun\": HookeJeevesOptimizer,\n",
    "        \"max_iter\": 17,\n",
    "        \"file_suffix\": \"hooke\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"L-BFGS (Gradient)\",\n",
    "        \"func\": k_fold_validation_gradient,   # Gradient specific function\n",
    "        \"Optimizer_fun\": LBFGSOptimizerWrapper,\n",
    "        \"max_iter\": 100,\n",
    "        \"file_suffix\": \"LBFGS\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762b3107-73a3-4e30-8201-1e32ff22378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "\n",
    "# Set up basic logging to track failures\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "# 1. SETUP: Define your Datasets, Models, and Target Variables here.\n",
    "# IMPORTANT: Check \"target_var\" for diabetes, heart, etc. matches your CSV headers.\n",
    "data_configs = [\n",
    "    {    \"name\": \"Australian\",\n",
    "        \"csv\": \"datasets/AustralianDisc.csv\",\n",
    "        \"xml\": \"models/australianNaive.xml\",\n",
    "        \"target_var\": \"A15\" \n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# 2. SETUP: Define your Algorithms\n",
    "optimizers = [\n",
    "    {\n",
    "        \"name\": \"L-BFGS (Gradient)\",\n",
    "        \"func\": k_fold_validation_gradient,   # Gradient specific function\n",
    "        \"Optimizer_fun\": LBFGSOptimizerWrapper,\n",
    "        \"max_iter\": 100,\n",
    "        \"file_suffix\": \"LBFGS\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 3. BUILD: Generate the list of experiments dynamically\n",
    "experiments = []\n",
    "\n",
    "for data in data_configs:\n",
    "    for opt in optimizers:\n",
    "        \n",
    "        # Auto-generate the results filename based on dataset and method\n",
    "        # Example: results_AustralianTAN_simplex.txt\n",
    "        result_name = f\"results_{data['name']}TAN_{opt['file_suffix']}.txt\"\n",
    "        \n",
    "        experiments.append({\n",
    "            \"name\": f\"{data['name']} - {opt['name']}\",\n",
    "            \"func\": opt['func'],\n",
    "            \"params\": {\n",
    "                \"xml_file\": data['xml'],\n",
    "                \"csv_file\": data['csv'],\n",
    "                \"target_var\": data['target_var'],\n",
    "                \"Optimizer_fun\": opt['Optimizer_fun'],\n",
    "                \"max_iter\": opt['max_iter'],\n",
    "                \"results_file\": result_name\n",
    "            }\n",
    "        })\n",
    "\n",
    "# 4. EXECUTE: Run the loop\n",
    "print(f\"Scheduled {len(experiments)} experiments.\\n\")\n",
    "\n",
    "for exp in experiments:\n",
    "    print(f\"--- Starting Experiment: {exp['name']} ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Unpack the dictionary parameters into the function\n",
    "        exp['func'](**exp['params'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR in {exp['name']}: {e}\")\n",
    "        logging.error(f\"Failed {exp['name']}: {e}\")\n",
    "        # 'continue' jumps to the next iteration of the for-loop\n",
    "        continue \n",
    "    \n",
    "    else:\n",
    "        # This only runs if the 'try' block succeeded\n",
    "        end_time = time.time()\n",
    "        elapsed_minutes = (end_time - start_time) / 60\n",
    "        print(f\"Successfully finished {exp['name']}.\")\n",
    "        print(f\"Saved to: {exp['params']['results_file']}\")\n",
    "        print(f\"Elapsed time: {elapsed_minutes:.2f} minutes\\n\")\n",
    "\n",
    "print(\"\\nAll scheduled experiments have been processed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
